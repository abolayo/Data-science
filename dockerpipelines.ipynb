{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dockerpipelines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DT02KADQx2DZ",
        "outputId": "d2cdd241-8780-4322-cbcc-f7ffa868e9ea"
      },
      "source": [
        "#installing kfp in notebook enviroment\n",
        "!python -m pip install --user --upgrade pip\n",
        "!pip3 install kfp --upgrade --user"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 32.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.7 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed pip-21.3.1\n",
            "Collecting kfp\n",
            "  Downloading kfp-1.8.6.tar.gz (266 kB)\n",
            "     |████████████████████████████████| 266 kB 27.5 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting absl-py<=0.11,>=0.9\n",
            "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "     |████████████████████████████████| 127 kB 49.2 MB/s            \n",
            "\u001b[?25hCollecting PyYAML<6,>=5.3\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "     |████████████████████████████████| 636 kB 40.4 MB/s            \n",
            "\u001b[?25hCollecting google-cloud-storage<2,>=1.20.0\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "     |████████████████████████████████| 105 kB 54.8 MB/s            \n",
            "\u001b[?25hCollecting kubernetes<19,>=8.0.0\n",
            "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
            "     |████████████████████████████████| 1.6 MB 47.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from kfp) (1.12.8)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from kfp) (1.35.0)\n",
            "Collecting requests-toolbelt<1,>=0.8.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "     |████████████████████████████████| 54 kB 2.3 MB/s             \n",
            "\u001b[?25hCollecting cloudpickle<3,>=2.0.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
            "  Downloading kfp-server-api-1.7.0.tar.gz (52 kB)\n",
            "     |████████████████████████████████| 52 kB 1.6 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonschema<4,>=3.0.1\n",
            "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
            "     |████████████████████████████████| 56 kB 4.0 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.6 in /usr/local/lib/python3.7/dist-packages (from kfp) (0.8.9)\n",
            "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from kfp) (7.1.2)\n",
            "Collecting Deprecated<2,>=1.2.7\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting strip-hints<1,>=0.1.8\n",
            "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docstring-parser<1,>=0.7.3\n",
            "  Downloading docstring_parser-0.12.tar.gz (23 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kfp-pipeline-spec<0.2.0,>=0.1.10\n",
            "  Downloading kfp_pipeline_spec-0.1.13-py3-none-any.whl (18 kB)\n",
            "Collecting fire<1,>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "     |████████████████████████████████| 87 kB 6.7 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from kfp) (3.17.3)\n",
            "Requirement already satisfied: uritemplate<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from kfp) (3.0.1)\n",
            "Collecting pydantic<2,>=1.8.2\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "     |████████████████████████████████| 10.1 MB 42.9 MB/s            \n",
            "\u001b[?25hCollecting typer<1.0,>=0.3.2\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from kfp) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py<=0.11,>=0.9->kfp) (1.15.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.12.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.1->kfp) (57.4.0)\n",
            "Collecting google-api-core<2dev,>=1.21.0\n",
            "  Downloading google_api_core-1.31.3-py2.py3-none-any.whl (93 kB)\n",
            "     |████████████████████████████████| 93 kB 1.7 MB/s             \n",
            "\u001b[?25hCollecting google-resumable-media<3.0dev,>=1.3.0\n",
            "  Downloading google_resumable_media-2.1.0-py2.py3-none-any.whl (75 kB)\n",
            "     |████████████████████████████████| 75 kB 5.0 MB/s             \n",
            "\u001b[?25hCollecting google-cloud-core<3.0dev,>=1.6.0\n",
            "  Downloading google_cloud_core-2.1.0-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.23.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp) (4.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<4,>=3.0.1->kfp) (21.2.0)\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
            "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "     |████████████████████████████████| 52 kB 1.6 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.7/dist-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (21.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2018.9)\n",
            "Collecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<2,>=1.20.0->kfp) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp) (3.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp) (2.4.7)\n",
            "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp: filename=kfp-1.8.6-py3-none-any.whl size=368187 sha256=606904c6c5c4e2062e8c05899e91f2ec9956d3360bd37f35d679399fbc3423ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/64/10/674d7031ce60fe302212184836d025331fc0592aee1e5ecd63\n",
            "  Building wheel for docstring-parser (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docstring-parser: filename=docstring_parser-0.12-py3-none-any.whl size=31768 sha256=1360656c27990f05353995c6c7407fa9ccdf6523530c6b7579e33281004412b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/19/f8/cd12288f40ba475ea882f7ed4c333039014e71e2c80bfd2a49\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=4b7b281b8b15d7d61d5d544ebc85b26b58ff46b5f18dd62099f05d1412d90d02\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-1.7.0-py3-none-any.whl size=92619 sha256=6924238585d37d39fc8c38737598c503e85b54384aa453f2be12667e94866c60\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/f0/36/cd1c7475b12b2541f90e4ab9413e59756a11262c1307a97633\n",
            "  Building wheel for strip-hints (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=e902d043c3cb6469a4075aef09bacf13c37512783620d95682145a3bc1515ffd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
            "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
            "Installing collected packages: google-crc32c, google-api-core, websocket-client, PyYAML, google-resumable-media, google-cloud-core, typer, strip-hints, requests-toolbelt, pydantic, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, google-cloud-storage, fire, docstring-parser, Deprecated, cloudpickle, absl-py, kfp\n",
            "\u001b[33m  WARNING: The script strip-hints is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script jsonschema is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.5.4 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.1.0 which is incompatible.\n",
            "google-cloud-bigquery 1.21.0 requires google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you have google-resumable-media 2.1.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 absl-py-0.11.0 cloudpickle-2.0.0 docstring-parser-0.12 fire-0.4.0 google-api-core-1.31.3 google-cloud-core-2.1.0 google-cloud-storage-1.42.3 google-crc32c-1.3.0 google-resumable-media-2.1.0 jsonschema-3.2.0 kfp-1.8.6 kfp-pipeline-spec-0.1.13 kfp-server-api-1.7.0 kubernetes-18.20.0 pydantic-1.8.2 requests-toolbelt-0.9.1 strip-hints-0.1.10 typer-0.4.0 websocket-client-1.2.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqKf-mu2wWSH"
      },
      "source": [
        "import kfp\n",
        "from kfp import dsl\n",
        "import kfp.components as comp"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1XIuZ3dzh15"
      },
      "source": [
        "def optain_data(data_path):\n",
        "  import pickle\n",
        "  import sys, subprocess;\n",
        "  subprocess.run([sys.executable, 'm', 'pip', 'install', 'pandas==0.23.4'])\n",
        "  import pandas as pd\n",
        "  \n",
        "  #reading the data from its source\n",
        "  data = pd.read_csv(\"https://raw.githubusercontent.com/MavenCode/KubeflowTraining/master/Data/Telco/Churn_Modelling.csv\")\n",
        "  #save the data as a pickle file to be used by the preprocessing components\n",
        "  with open(f'{data_path}/working_data', 'wb') as f:\n",
        "    pickle.dump(data,f)\n",
        "  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afEMGgSb7gGw"
      },
      "source": [
        "def preprocessing(data_path):\n",
        "  import pickle\n",
        "  import sys, subprocess;\n",
        "  subprocess.run([sys.executable, 'm', 'pip', 'install', 'pandas==0.23.4'])\n",
        "  subprocess.run([sys.executable, 'm', 'pip', 'install', 'scikit-learn==0.22'])\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "   #load the working data\n",
        "  with open(f'{data_path}/working_data', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    \n",
        "  #drop columns that are not needed\n",
        "  data = data.drop(columns=['CustomerId','Surname','RowNumber'],axis=1)\n",
        "\n",
        "  #independent features\n",
        "  X = data.iloc[:,:-1]\n",
        "\n",
        "  #dependent feature\n",
        "  y = data.iloc[:,-1:]\n",
        "\n",
        "  #use onehot encoder and label encoder for the categorical features\n",
        "  le = LabelEncoder()\n",
        "  ohe = OneHotEncoder()\n",
        "\n",
        "  X['Gender']= le.fit_transform(X['Gender'])\n",
        "  geo_df = pd.DataFrame(ohe.fit_transform(X[['Geography']]).toarray())\n",
        "\n",
        "  #getting feature name after onehotencoding\n",
        "  geo_df.columns = ohe.get_feature_names(['Geography'])\n",
        "\n",
        "  #merging geo_df with the main data\n",
        "  X = X.join(geo_df)\n",
        "\n",
        "  #drop redundant column\n",
        "  X = X.drop(columns=['Geography'],axis=1)\n",
        "\n",
        "  #split data\n",
        "  X_train,X_test,y_train,y_test = train_test_split( X,y, test_size=0.2, random_state = 42)\n",
        "\n",
        "  #data scaling\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  min_max_scaler = MinMaxScaler(feature_range =(0, 1))\n",
        "  \n",
        "  # Scaled feature\n",
        "  X_train = min_max_scaler.fit_transform(X_train)\n",
        "  X_test = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "  sc =StandardScaler()\n",
        "  X_train = sc.fit_transform(X_train)\n",
        "  X_test = sc.transform(X_test)\n",
        "  #Save the train data as a pickle file to be used for train component\n",
        "  with open(f'{data_path}/train_data', 'wb') as f:\n",
        "    pickle.dump((X_train, y_train),f)\n",
        "     #Save the train data as a pickle file to be used for train component\n",
        "  with open(f'{data_path}/test_data', 'wb') as f:\n",
        "    pickle.dump((X_test, y_test),f)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_3CgaoL4HDn"
      },
      "source": [
        "def train_tensorflow(data_path,train_data,model):\n",
        "  import pickle\n",
        "  #import library\n",
        "  import numpy as np\n",
        "  from tensorflow import keras\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense\n",
        "\n",
        "  #loading the train data\n",
        "  with open(f'{data_path}/{train-data}', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "  #seperate the X_train from y_train.\n",
        "  X_train, y_train = train_data\n",
        "\n",
        "  #initializing the classifier model with its input, and output layer\n",
        "  #using keras sequential model\n",
        "  #initializing the classifier model with its input, hidden and output layers\n",
        "  classifier = Sequential()\n",
        "  classifier.add(Dense(units = 16, activation='relu', input_dim=12,))\n",
        "  classifier.add(Dense(units = 8, activation='relu'))\n",
        "  classifier.add(Dense(units = 1, activation='sigmoid'))\n",
        "  #Compiling the classifier model with Stochastics Gradient Descent\n",
        "  classifier.compile(optimizer = 'adam', loss='binary_crossentropy' , metrics =['accuracy']) \n",
        "  #fitting the model\n",
        "  classifier.fit(X_train,y_train,batch_size=10, epochs=150)\n",
        "  #saving the model\n",
        "  classifier.save(f'{data_path}/{model}')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUlWBP0YWRKb"
      },
      "source": [
        "def predict_tensorflow(data_path,test_data,model):\n",
        "    import pickle\n",
        "    import numpy as np\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.models import load_model\n",
        "\n",
        "    #loading the X_test and y_test\n",
        "    with open(f'{data_path}/{test_data}', 'rb') as f:\n",
        "      test_data = pickle.load(f)\n",
        "    # Seperate the X_test from y_test.\n",
        "    X_test, y_test = test_data\n",
        "    #loading the model\n",
        "    classifier = load_model(f'{data_path}/{model}')\n",
        "    #Evaluation the model and print the results\n",
        "    test_loss, test_acc = classifier.evaluation(X_test, y_test, verbose=0)\n",
        "    #model's prediction on test date\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    # create a threshold for the confution matrics\n",
        "    y_pred=(y_pred>0.5)\n",
        "\n",
        "    #saving the test_loss and test_acc\n",
        "    with open(f'{data_path}/performance.txt', 'w') as f:\n",
        "      f.write(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
        "\n",
        "    #saving the predictions\n",
        "    with open(f'{data_path}/results.txt', 'w') as result:\n",
        "      result.write(\" Prediction: {}, Actual: {} \".format(y_pred,y_test.astype(np.bool))) \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_oipZJTgoDN"
      },
      "source": [
        "#create light-weight components\n",
        "obtain_data_op = kfp.components.create_component_from_func(obtain_data,base_image=\"python:3.7.1\")\n",
        "preprocess_op = ktp.components.create_component_from_func(preprocess,base_image=\"python:3.7.1\")\n",
        "train_op = ktp.components.create_component_from_func(train_tensorflow,base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
        "predict_op = ktp.components.create_component_from_func(predict_tensorflow,base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOJw8MtZ1GrV"
      },
      "source": [
        "**Define tensorflow pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLY3umKexrJV"
      },
      "source": [
        "from kfp.components import create_component_from_func\n",
        "#create a cleint that will enable communication with pipeline API\n",
        "client = kfp.Client()\n",
        "# Define pipeline\n",
        "@dsl.pipeline(name=\"Churn Pipeline\", description=\"Performs Preprocessing, training and prediction of churn rate\")\n",
        "#Define parameters to be fed into thr pipeline\n",
        "def churn_lightweight_tensorflow_pipeline(data_path:str,\n",
        "                                          working_data: str,\n",
        "                                          train_data: str,\n",
        "                                          test_data: str,\n",
        "                                          model:str):\n",
        "  #define volume to share data between components\n",
        "  volume_op = dsl.VolumeOp(\n",
        "      name=\"data_volume\",\n",
        "      resource_name=\"data-volume\",\n",
        "      size=\"1Gi\",\n",
        "      modes=dsl.VOLUME_MODE_RWO\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}